{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraphæ ¸å¿ƒæ¶æ„å®ç°\n",
    "## 13å¤©æ–°ç–†æ—…æ¸¸è§„åˆ’ - æ™ºèƒ½çŠ¶æ€å›¾è®¾è®¡\n",
    "\n",
    "**ç›®æ ‡**: å®ç°åŸºäºLangGraphçš„æ™ºèƒ½æ—…æ¸¸è§„åˆ’çŠ¶æ€å›¾ï¼Œè§£å†³Tokené™åˆ¶é—®é¢˜\n",
    "\n",
    "**æ ¸å¿ƒåˆ›æ–°**:\n",
    "- åœ°ç†åˆ†ç‰‡ç­–ç•¥ï¼šå°†13å¤©è§„åˆ’åˆ†è§£ä¸º4ä¸ªåŒºåŸŸ\n",
    "- æ™ºèƒ½çŠ¶æ€ç®¡ç†ï¼šæ”¯æŒä¸­æ–­æ¢å¤å’Œé”™è¯¯å¤„ç†\n",
    "- æ¡ä»¶è·¯ç”±ï¼šæ ¹æ®æ•°æ®è´¨é‡åŠ¨æ€é€‰æ‹©å¤„ç†è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„ä¾èµ–\n",
    "!pip install langgraph langchain-core typing-extensions pydantic openai aiohttp asyncio requests tenacity tiktoken jinja2 nest-asyncio python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¯å¢ƒå˜é‡é…ç½®å’ŒAPIå¯†é’¥åŠ è½½\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "# è·å–APIå¯†é’¥å’Œé…ç½®\n",
    "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "AMAP_MCP_API_KEY = os.getenv('AMAP_MCP_API_KEY')\n",
    "AMAP_MCP_BASE_URL = os.getenv('AMAP_MCP_BASE_URL', 'http://localhost:8080/mcp')\n",
    "DEEPSEEK_API_BASE_URL = os.getenv('DEEPSEEK_API_BASE_URL', 'https://api.deepseek.com/v1')\n",
    "DEEPSEEK_MODEL = os.getenv('DEEPSEEK_MODEL', 'deepseek-chat')\n",
    "\n",
    "# LangGraphé…ç½®\n",
    "LANGGRAPH_TIMEOUT = int(os.getenv('LANGGRAPH_TIMEOUT', '300'))\n",
    "MAX_ITERATIONS = int(os.getenv('LANGGRAPH_MAX_ITERATIONS', '10'))\n",
    "COMPLEXITY_THRESHOLD_SIMPLE = int(os.getenv('COMPLEXITY_THRESHOLD_SIMPLE', '30'))\n",
    "COMPLEXITY_THRESHOLD_MEDIUM = int(os.getenv('COMPLEXITY_THRESHOLD_MEDIUM', '60'))\n",
    "COMPLEXITY_THRESHOLD_COMPLEX = int(os.getenv('COMPLEXITY_THRESHOLD_COMPLEX', '100'))\n",
    "\n",
    "# éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡\n",
    "if not DEEPSEEK_API_KEY:\n",
    "    print(\"âš ï¸ DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼\")\n",
    "if not AMAP_MCP_API_KEY:\n",
    "    print(\"âš ï¸ AMAP_MCP_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®\")\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ\")\n",
    "print(f\"ğŸ”‘ DeepSeek API: {DEEPSEEK_API_BASE_URL}\")\n",
    "print(f\"ğŸ—ºï¸ é«˜å¾·MCP: {AMAP_MCP_BASE_URL}\")\n",
    "print(f\"ğŸ¤– AIæ¨¡å‹: {DEEPSEEK_MODEL}\")\n",
    "print(f\"â±ï¸ è¶…æ—¶è®¾ç½®: {LANGGRAPH_TIMEOUT}ç§’\")\n",
    "\n",
    "# é…ç½®æ—¥å¿—\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Optional, Any, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime, timedelta\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DeepSeek APIå®¢æˆ·ç«¯\n",
    "\n",
    "é›†æˆDeepSeek APIï¼Œæ›¿æ¢OpenAI APIè°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "class DeepSeekAPIClient:\n",
    "    \"\"\"DeepSeek APIå®¢æˆ·ç«¯ - å…¼å®¹OpenAIæ¥å£\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None, base_url: str = None, model: str = None):\n",
    "        self.api_key = api_key or DEEPSEEK_API_KEY\n",
    "        self.base_url = base_url or DEEPSEEK_API_BASE_URL\n",
    "        self.model = model or DEEPSEEK_MODEL\n",
    "        \n",
    "        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ŒæŒ‡å‘DeepSeekç«¯ç‚¹\n",
    "        if self.api_key:\n",
    "            self.client = OpenAI(\n",
    "                api_key=self.api_key,\n",
    "                base_url=self.base_url\n",
    "            )\n",
    "        else:\n",
    "            self.client = None\n",
    "            logger.warning(\"âš ï¸ DeepSeek APIå¯†é’¥æœªè®¾ç½®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼\")\n",
    "        \n",
    "        # é…ç½®å‚æ•°\n",
    "        self.max_tokens = int(os.getenv('DEEPSEEK_MAX_TOKENS', '4000'))\n",
    "        self.temperature = float(os.getenv('DEEPSEEK_TEMPERATURE', '0.7'))\n",
    "        self.top_p = float(os.getenv('DEEPSEEK_TOP_P', '0.95'))\n",
    "        self.max_retries = int(os.getenv('MAX_RETRIES', '3'))\n",
    "        \n",
    "        logger.info(f\"ğŸ¤– DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ: {self.model}\")\n",
    "    \n",
    "    def sync_chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"åŒæ­¥èŠå¤©å®ŒæˆAPIè°ƒç”¨\"\"\"\n",
    "        if not self.client:\n",
    "            # æ¨¡æ‹Ÿå“åº”\n",
    "            return {\n",
    "                'content': f\"æ¨¡æ‹ŸAIå“åº” - åŸºäºè¾“å…¥æ¶ˆæ¯çš„æ™ºèƒ½åˆ†æç»“æœ\",\n",
    "                'usage': {'total_tokens': 100, 'prompt_tokens': 50, 'completion_tokens': 50},\n",
    "                'model': self.model,\n",
    "                'finish_reason': 'stop'\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            params = {\n",
    "                'model': self.model,\n",
    "                'messages': messages,\n",
    "                'max_tokens': self.max_tokens,\n",
    "                'temperature': self.temperature,\n",
    "                'top_p': self.top_p,\n",
    "                **kwargs\n",
    "            }\n",
    "            \n",
    "            response = self.client.chat.completions.create(**params)\n",
    "            \n",
    "            logger.info(f\"âœ… DeepSeek APIè°ƒç”¨æˆåŠŸ\")\n",
    "            return {\n",
    "                'content': response.choices[0].message.content,\n",
    "                'usage': response.usage.dict() if response.usage else {},\n",
    "                'model': response.model,\n",
    "                'finish_reason': response.choices[0].finish_reason\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {e}\")\n",
    "            # è¿”å›æ¨¡æ‹Ÿå“åº”ä»¥ä¿è¯æµç¨‹ç»§ç»­\n",
    "            return {\n",
    "                'content': f\"æ¨¡æ‹ŸAIå“åº” - ç”±äºAPIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘å¤„ç†\",\n",
    "                'usage': {'total_tokens': 100, 'prompt_tokens': 50, 'completion_tokens': 50},\n",
    "                'model': self.model,\n",
    "                'finish_reason': 'stop'\n",
    "            }\n",
    "\n",
    "# åˆ›å»ºå…¨å±€DeepSeekå®¢æˆ·ç«¯å®ä¾‹\n",
    "deepseek_client = DeepSeekAPIClient()\n",
    "print(f\"ğŸš€ DeepSeekå®¢æˆ·ç«¯å°±ç»ª: {deepseek_client.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ ¸å¿ƒçŠ¶æ€å®šä¹‰\n",
    "\n",
    "åŸºäºç¬¬ä¸€æ€§åŸç†è®¾è®¡çš„çŠ¶æ€ç»“æ„ï¼Œæ”¯æŒ13å¤©å¤æ‚è§„åˆ’çš„åˆ†ç‰‡å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserPreferences(BaseModel):\n",
    "    \"\"\"ç”¨æˆ·åå¥½æ•°æ®ç»“æ„\"\"\"\n",
    "    budget_level: str = Field(description=\"é¢„ç®—ç­‰çº§: budget/mid/luxury\")\n",
    "    travel_style: List[str] = Field(description=\"æ—…è¡Œé£æ ¼: æ–‡åŒ–/è‡ªç„¶/ç¾é£Ÿ/æ‘„å½±\")\n",
    "    group_size: int = Field(description=\"å›¢é˜Ÿäººæ•°\")\n",
    "    special_requirements: Optional[str] = Field(description=\"ç‰¹æ®Šéœ€æ±‚\")\n",
    "    interests: List[str] = Field(description=\"å…´è¶£ç‚¹\")\n",
    "\n",
    "class RegionInfo(BaseModel):\n",
    "    \"\"\"åŒºåŸŸä¿¡æ¯\"\"\"\n",
    "    name: str = Field(description=\"åŒºåŸŸåç§°\")\n",
    "    days: int = Field(description=\"åœç•™å¤©æ•°\")\n",
    "    priority: int = Field(description=\"ä¼˜å…ˆçº§ 1-4\")\n",
    "    key_attractions: List[str] = Field(description=\"æ ¸å¿ƒæ™¯ç‚¹\")\n",
    "    estimated_tokens: int = Field(description=\"é¢„ä¼°Tokenä½¿ç”¨é‡\")\n",
    "\n",
    "class ProcessingError(BaseModel):\n",
    "    \"\"\"å¤„ç†é”™è¯¯ä¿¡æ¯\"\"\"\n",
    "    node_name: str\n",
    "    error_type: str\n",
    "    message: str\n",
    "    timestamp: datetime\n",
    "    retryable: bool = True\n",
    "\n",
    "class TravelPlanningState(TypedDict):\n",
    "    \"\"\"LangGraphçŠ¶æ€å®šä¹‰ - 13å¤©æ–°ç–†è§„åˆ’ä¸“ç”¨\"\"\"\n",
    "    # åŸºç¡€ä¿¡æ¯\n",
    "    session_id: str\n",
    "    user_preferences: UserPreferences\n",
    "    destination: str  # \"æ–°ç–†\"\n",
    "    total_days: int   # 13\n",
    "    start_date: str\n",
    "    \n",
    "    # åˆ†ç‰‡ä¿¡æ¯\n",
    "    regions: List[RegionInfo]\n",
    "    current_region_index: int\n",
    "    current_phase: str  # \"analyze\" | \"collect_data\" | \"plan_region\" | \"merge\" | \"finalize\"\n",
    "    \n",
    "    # æ•°æ®å±‚\n",
    "    real_data: Dict[str, Any]  # é«˜å¾·MCPæ•°æ®\n",
    "    region_plans: Dict[str, Any]  # å„åŒºåŸŸçš„è¯¦ç»†è§„åˆ’\n",
    "    \n",
    "    # æœ€ç»ˆç»“æœ\n",
    "    master_plan: Optional[Dict[str, Any]]\n",
    "    html_output: Optional[str]\n",
    "    \n",
    "    # æ‰§è¡ŒçŠ¶æ€\n",
    "    progress: float\n",
    "    errors: List[ProcessingError]\n",
    "    retry_count: int\n",
    "    quality_score: float\n",
    "    \n",
    "    # Tokenç®¡ç†\n",
    "    tokens_used: int\n",
    "    tokens_remaining: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ ¸å¿ƒèŠ‚ç‚¹å®ç°\n",
    "\n",
    "æ¯ä¸ªèŠ‚ç‚¹éƒ½ä¸“æ³¨äºå•ä¸€èŒè´£ï¼Œç¡®ä¿Tokenä½¿ç”¨å¯æ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_complexity_node(state: TravelPlanningState) -> TravelPlanningState:\n",
    "    \"\"\"åˆ†æ13å¤©è§„åˆ’å¤æ‚åº¦ï¼Œç¡®å®šåˆ†ç‰‡ç­–ç•¥\"\"\"\n",
    "    print(f\"ğŸ” åˆ†æå¤æ‚åº¦ - ä¼šè¯ID: {state['session_id']}\")\n",
    "    \n",
    "    # å¤æ‚åº¦è¯„åˆ†ç®—æ³•\n",
    "    complexity_score = 0\n",
    "    complexity_score += state['total_days'] * 5  # 13å¤© = 65åˆ†\n",
    "    complexity_score += len(state['user_preferences'].interests) * 3  # å…´è¶£ç‚¹\n",
    "    complexity_score += state['user_preferences'].group_size * 2  # å›¢é˜Ÿè§„æ¨¡\n",
    "    \n",
    "    print(f\"ğŸ“Š å¤æ‚åº¦è¯„åˆ†: {complexity_score}\")\n",
    "    \n",
    "    # æ›´æ–°çŠ¶æ€\n",
    "    state['current_phase'] = 'analyze'\n",
    "    state['progress'] = 10.0\n",
    "    \n",
    "    # è®°å½•åˆ†æç»“æœ\n",
    "    if 'analysis_result' not in state:\n",
    "        state['analysis_result'] = {}\n",
    "    \n",
    "    state['analysis_result']['complexity_score'] = complexity_score\n",
    "    state['analysis_result']['strategy'] = 'comprehensive' if complexity_score > 80 else 'standard'\n",
    "    \n",
    "    print(f\"âœ… å¤æ‚åº¦åˆ†æå®Œæˆï¼Œç­–ç•¥: {state['analysis_result']['strategy']}\")\n",
    "    return state\n",
    "\n",
    "def region_decomposition_node(state: TravelPlanningState) -> TravelPlanningState:\n",
    "    \"\"\"å°†æ–°ç–†åˆ†è§£ä¸º4ä¸ªæ ¸å¿ƒåŒºåŸŸ\"\"\"\n",
    "    print(f\"ğŸ—ºï¸ åŒºåŸŸåˆ†è§£ - ç›®æ ‡: {state['destination']}\")\n",
    "    \n",
    "    # æ–°ç–†4å¤§åŒºåŸŸåˆ†è§£ç­–ç•¥\n",
    "    xinjiang_regions = [\n",
    "        RegionInfo(\n",
    "            name=\"ä¹Œé²æœ¨é½\",\n",
    "            days=3,\n",
    "            priority=1,\n",
    "            key_attractions=[\"å¤©å±±å¤©æ± \", \"æ–°ç–†åšç‰©é¦†\", \"çº¢å±±å…¬å›­\", \"å¤§å·´æ‰\"],\n",
    "            estimated_tokens=2500\n",
    "        ),\n",
    "        RegionInfo(\n",
    "            name=\"å–€ä»€\",\n",
    "            days=4,\n",
    "            priority=2,\n",
    "            key_attractions=[\"å–€ä»€å¤åŸ\", \"è‰¾æå°•å°”æ¸…çœŸå¯º\", \"é¦™å¦ƒå¢“\", \"å¸•ç±³å°”é«˜åŸ\"],\n",
    "            estimated_tokens=3200\n",
    "        ),\n",
    "        RegionInfo(\n",
    "            name=\"ä¼ŠçŠ\",\n",
    "            days=3,\n",
    "            priority=3,\n",
    "            key_attractions=[\"é‚£æ‹‰æè‰åŸ\", \"è–°è¡£è‰åŸºåœ°\", \"èµ›é‡Œæœ¨æ¹–\", \"æœå­æ²Ÿ\"],\n",
    "            estimated_tokens=2500\n",
    "        ),\n",
    "        RegionInfo(\n",
    "            name=\"åé²ç•ª\",\n",
    "            days=3,\n",
    "            priority=4,\n",
    "            key_attractions=[\"ç«ç„°å±±\", \"è‘¡è„æ²Ÿ\", \"äº¤æ²³æ•…åŸ\", \"åå„¿äº•\"],\n",
    "            estimated_tokens=2500\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # æ ¹æ®ç”¨æˆ·åå¥½è°ƒæ•´åŒºåŸŸä¼˜å…ˆçº§\n",
    "    if \"è‡ªç„¶\" in state['user_preferences'].travel_style:\n",
    "        # æé«˜ä¼ŠçŠä¼˜å…ˆçº§\n",
    "        for region in xinjiang_regions:\n",
    "            if region.name == \"ä¼ŠçŠ\":\n",
    "                region.priority = 1\n",
    "    \n",
    "    if \"æ–‡åŒ–\" in state['user_preferences'].travel_style:\n",
    "        # æé«˜å–€ä»€ä¼˜å…ˆçº§\n",
    "        for region in xinjiang_regions:\n",
    "            if region.name == \"å–€ä»€\":\n",
    "                region.priority = 1\n",
    "    \n",
    "    # æŒ‰ä¼˜å…ˆçº§æ’åº\n",
    "    xinjiang_regions.sort(key=lambda x: x.priority)\n",
    "    \n",
    "    # æ›´æ–°çŠ¶æ€\n",
    "    state['regions'] = xinjiang_regions\n",
    "    state['current_region_index'] = 0\n",
    "    state['current_phase'] = 'decomposition'\n",
    "    state['progress'] = 20.0\n",
    "    \n",
    "    # è®¡ç®—æ€»Tokené¢„ä¼°\n",
    "    total_tokens = sum(region.estimated_tokens for region in xinjiang_regions)\n",
    "    state['tokens_remaining'] = total_tokens\n",
    "    \n",
    "    print(f\"ğŸ“ åŒºåŸŸåˆ†è§£å®Œæˆ:\")\n",
    "    for i, region in enumerate(xinjiang_regions):\n",
    "        print(f\"  {i+1}. {region.name} ({region.days}å¤©) - ä¼˜å…ˆçº§{region.priority}\")\n",
    "    print(f\"ğŸ’° é¢„ä¼°Tokenæ€»é‡: {total_tokens}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¡ä»¶è·¯ç”±å‡½æ•°\n",
    "\n",
    "æ™ºèƒ½å†³ç­–æœºåˆ¶ï¼Œæ ¹æ®çŠ¶æ€åŠ¨æ€é€‰æ‹©å¤„ç†è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_regions(state: TravelPlanningState) -> str:\n",
    "    \"\"\"åˆ¤æ–­æ˜¯å¦ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªåŒºåŸŸ\"\"\"\n",
    "    current_index = state.get('current_region_index', 0)\n",
    "    total_regions = len(state.get('regions', []))\n",
    "    \n",
    "    print(f\"ğŸ”„ è·¯ç”±æ£€æŸ¥: å½“å‰åŒºåŸŸ {current_index + 1}/{total_regions}\")\n",
    "    \n",
    "    if current_index < total_regions - 1:\n",
    "        print(f\"â¡ï¸ ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªåŒºåŸŸ\")\n",
    "        return \"collect_region_data\"\n",
    "    else:\n",
    "        print(f\"ğŸ”— æ‰€æœ‰åŒºåŸŸå®Œæˆï¼Œå¼€å§‹åˆå¹¶\")\n",
    "        return \"merge_regions\"\n",
    "\n",
    "def should_retry_region(state: TravelPlanningState) -> str:\n",
    "    \"\"\"åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•å½“å‰åŒºåŸŸ\"\"\"\n",
    "    retry_count = state.get('retry_count', 0)\n",
    "    quality_score = state.get('quality_score', 0.0)\n",
    "    \n",
    "    print(f\"ğŸ” è´¨é‡æ£€æŸ¥: è¯„åˆ† {quality_score:.2f}, é‡è¯•æ¬¡æ•° {retry_count}\")\n",
    "    \n",
    "    if retry_count < 3 and quality_score < 0.7:\n",
    "        print(f\"ğŸ”„ è´¨é‡ä¸è¾¾æ ‡ï¼Œé‡è¯•åŒºåŸŸè§„åˆ’\")\n",
    "        return \"plan_region\"\n",
    "    else:\n",
    "        print(f\"âœ… è´¨é‡è¾¾æ ‡æˆ–é‡è¯•æ¬¡æ•°å·²æ»¡ï¼Œç»§ç»­ä¸‹ä¸€æ­¥\")\n",
    "        return \"next_region\"\n",
    "\n",
    "def route_by_data_quality(state: TravelPlanningState) -> str:\n",
    "    \"\"\"æ ¹æ®æ•°æ®è´¨é‡é€‰æ‹©å¤„ç†è·¯å¾„\"\"\"\n",
    "    real_data = state.get('real_data', {})\n",
    "    current_region = state['regions'][state['current_region_index']]\n",
    "    \n",
    "    # æ£€æŸ¥å½“å‰åŒºåŸŸçš„æ•°æ®å®Œæ•´æ€§\n",
    "    region_data = real_data.get(current_region.name, {})\n",
    "    \n",
    "    data_completeness = 0.0\n",
    "    if region_data.get('attractions'):\n",
    "        data_completeness += 0.4\n",
    "    if region_data.get('restaurants'):\n",
    "        data_completeness += 0.3\n",
    "    if region_data.get('hotels'):\n",
    "        data_completeness += 0.2\n",
    "    if region_data.get('weather'):\n",
    "        data_completeness += 0.1\n",
    "    \n",
    "    print(f\"ğŸ“Š æ•°æ®å®Œæ•´æ€§: {data_completeness:.1%}\")\n",
    "    \n",
    "    if data_completeness >= 0.7:\n",
    "        return \"plan_region\"\n",
    "    elif data_completeness >= 0.4:\n",
    "        return \"plan_region_basic\"\n",
    "    else:\n",
    "        return \"use_fallback_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ„å»ºLangGraphçŠ¶æ€å›¾\n",
    "\n",
    "æ ¸å¿ƒæ¶æ„å®ç°ï¼Œæ”¯æŒæ™ºèƒ½è·¯ç”±å’Œé”™è¯¯æ¢å¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å ä½ç¬¦èŠ‚ç‚¹å‡½æ•° - å°†åœ¨åç»­notebookä¸­å®ç°\n",
    "def collect_region_data_node(state: TravelPlanningState) -> TravelPlanningState:\n",
    "    \"\"\"æ”¶é›†å½“å‰åŒºåŸŸçš„é«˜å¾·MCPæ•°æ® - å ä½ç¬¦\"\"\"\n",
    "    print(f\"ğŸ“¡ æ•°æ®æ”¶é›†èŠ‚ç‚¹ - å ä½ç¬¦å®ç°\")\n",
    "    state['current_phase'] = 'collect_data'\n",
    "    state['progress'] = 30.0\n",
    "    return state\n",
    "\n",
    "def plan_region_node(state: TravelPlanningState) -> TravelPlanningState:\n",
    "    \"\"\"ä¸ºå½“å‰åŒºåŸŸç”Ÿæˆè¯¦ç»†è§„åˆ’ - å ä½ç¬¦\"\"\"\n",
    "    print(f\"ğŸ¯ åŒºåŸŸè§„åˆ’èŠ‚ç‚¹ - å ä½ç¬¦å®ç°\")\n",
    "    state['current_phase'] = 'plan_region'\n",
    "    state['progress'] = 50.0\n",
    "    return state\n",
    "\n",
    "def validate_region_node(state: TravelPlanningState) -> TravelPlanningState:\n",
    "    \"\"\"éªŒè¯åŒºåŸŸè§„åˆ’è´¨é‡ - å ä½ç¬¦\"\"\"\n",
    "    print(f\"âœ… è´¨é‡éªŒè¯èŠ‚ç‚¹ - å ä½ç¬¦å®ç°\")\n",
    "    state['quality_score'] = 0.85  # æ¨¡æ‹Ÿé«˜è´¨é‡\n",
    "    state['current_region_index'] += 1  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªåŒºåŸŸ\n",
    "    return state\n",
    "\n",
    "def merge_regions_node(state: TravelPlanningState) -> TravelPlanningState:\n",
    "    \"\"\"åˆå¹¶æ‰€æœ‰åŒºåŸŸè§„åˆ’ - å ä½ç¬¦\"\"\"\n",
    "    print(f\"ğŸ”— åŒºåŸŸåˆå¹¶èŠ‚ç‚¹ - å ä½ç¬¦å®ç°\")\n",
    "    state['current_phase'] = 'merge'\n",
    "    state['progress'] = 80.0\n",
    "    return state\n",
    "\n",
    "def optimize_transitions_node(state: TravelPlanningState) -> TravelPlanningState:\n",
    "    \"\"\"ä¼˜åŒ–åŒºåŸŸé—´è½¬æ¢ - å ä½ç¬¦\"\"\"\n",
    "    print(f\"âš¡ è½¬æ¢ä¼˜åŒ–èŠ‚ç‚¹ - å ä½ç¬¦å®ç°\")\n",
    "    state['progress'] = 90.0\n",
    "    return state\n",
    "\n",
    "def generate_final_output_node(state: TravelPlanningState) -> TravelPlanningState:\n",
    "    \"\"\"ç”Ÿæˆæœ€ç»ˆè¾“å‡º - å ä½ç¬¦\"\"\"\n",
    "    print(f\"ğŸ“„ æœ€ç»ˆè¾“å‡ºèŠ‚ç‚¹ - å ä½ç¬¦å®ç°\")\n",
    "    state['current_phase'] = 'completed'\n",
    "    state['progress'] = 100.0\n",
    "    return state\n",
    "\n",
    "def build_xinjiang_planning_graph():\n",
    "    \"\"\"æ„å»º13å¤©æ–°ç–†æ—…æ¸¸è§„åˆ’çš„LangGraphçŠ¶æ€å›¾\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºçŠ¶æ€å›¾\n",
    "    workflow = StateGraph(TravelPlanningState)\n",
    "    \n",
    "    # æ·»åŠ æ ¸å¿ƒèŠ‚ç‚¹\n",
    "    workflow.add_node(\"analyze_complexity\", analyze_complexity_node)\n",
    "    workflow.add_node(\"region_decomposition\", region_decomposition_node)\n",
    "    workflow.add_node(\"collect_region_data\", collect_region_data_node)\n",
    "    workflow.add_node(\"plan_region\", plan_region_node)\n",
    "    workflow.add_node(\"validate_region\", validate_region_node)\n",
    "    workflow.add_node(\"merge_regions\", merge_regions_node)\n",
    "    workflow.add_node(\"optimize_transitions\", optimize_transitions_node)\n",
    "    workflow.add_node(\"generate_final_output\", generate_final_output_node)\n",
    "    \n",
    "    # è®¾ç½®å…¥å£ç‚¹\n",
    "    workflow.set_entry_point(\"analyze_complexity\")\n",
    "    \n",
    "    # æ·»åŠ å›ºå®šè¾¹\n",
    "    workflow.add_edge(\"analyze_complexity\", \"region_decomposition\")\n",
    "    workflow.add_edge(\"region_decomposition\", \"collect_region_data\")\n",
    "    workflow.add_edge(\"collect_region_data\", \"plan_region\")\n",
    "    workflow.add_edge(\"plan_region\", \"validate_region\")\n",
    "    \n",
    "    # æ·»åŠ æ¡ä»¶è¾¹ - åŒºåŸŸå¤„ç†å¾ªç¯\n",
    "    workflow.add_conditional_edges(\n",
    "        \"validate_region\",\n",
    "        should_continue_regions,\n",
    "        {\n",
    "            \"collect_region_data\": \"collect_region_data\",\n",
    "            \"merge_regions\": \"merge_regions\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # æ·»åŠ å›ºå®šè¾¹ - æœ€ç»ˆå¤„ç†\n",
    "    workflow.add_edge(\"merge_regions\", \"optimize_transitions\")\n",
    "    workflow.add_edge(\"optimize_transitions\", \"generate_final_output\")\n",
    "    workflow.add_edge(\"generate_final_output\", END)\n",
    "    \n",
    "    print(\"ğŸ—ï¸ LangGraphçŠ¶æ€å›¾æ„å»ºå®Œæˆ\")\n",
    "    print(\"ğŸ“‹ èŠ‚ç‚¹åˆ—è¡¨:\")\n",
    "    print(\"  1. analyze_complexity - å¤æ‚åº¦åˆ†æ\")\n",
    "    print(\"  2. region_decomposition - åŒºåŸŸåˆ†è§£\")\n",
    "    print(\"  3. collect_region_data - æ•°æ®æ”¶é›†\")\n",
    "    print(\"  4. plan_region - åŒºåŸŸè§„åˆ’\")\n",
    "    print(\"  5. validate_region - è´¨é‡éªŒè¯\")\n",
    "    print(\"  6. merge_regions - åŒºåŸŸåˆå¹¶\")\n",
    "    print(\"  7. optimize_transitions - è½¬æ¢ä¼˜åŒ–\")\n",
    "    print(\"  8. generate_final_output - æœ€ç»ˆè¾“å‡º\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# æµ‹è¯•çŠ¶æ€å›¾æ„å»º\n",
    "try:\n",
    "    planning_graph = build_xinjiang_planning_graph()\n",
    "    print(\"\\nâœ… çŠ¶æ€å›¾ç¼–è¯‘æˆåŠŸï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ çŠ¶æ€å›¾æ„å»ºå¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åˆå§‹åŒ–æµ‹è¯•\n",
    "\n",
    "éªŒè¯æ ¸å¿ƒæ¶æ„çš„å¯ç”¨æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_state() -> TravelPlanningState:\n",
    "    \"\"\"åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå§‹çŠ¶æ€\"\"\"\n",
    "    \n",
    "    test_preferences = UserPreferences(\n",
    "        budget_level=\"mid\",\n",
    "        travel_style=[\"æ–‡åŒ–\", \"è‡ªç„¶\", \"æ‘„å½±\"],\n",
    "        group_size=2,\n",
    "        special_requirements=\"å¸Œæœ›ä½“éªŒå½“åœ°æ°‘ä¿—æ–‡åŒ–\",\n",
    "        interests=[\"å†å²æ–‡åŒ–\", \"è‡ªç„¶é£å…‰\", \"ç¾é£Ÿä½“éªŒ\"]\n",
    "    )\n",
    "    \n",
    "    return TravelPlanningState(\n",
    "        session_id=f\"test_session_{int(time.time())}\",\n",
    "        user_preferences=test_preferences,\n",
    "        destination=\"æ–°ç–†\",\n",
    "        total_days=13,\n",
    "        start_date=\"2024-06-01\",\n",
    "        regions=[],\n",
    "        current_region_index=0,\n",
    "        current_phase=\"init\",\n",
    "        real_data={},\n",
    "        region_plans={},\n",
    "        master_plan=None,\n",
    "        html_output=None,\n",
    "        progress=0.0,\n",
    "        errors=[],\n",
    "        retry_count=0,\n",
    "        quality_score=0.0,\n",
    "        tokens_used=0,\n",
    "        tokens_remaining=0\n",
    "    )\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•çŠ¶æ€\n",
    "test_state = create_test_state()\n",
    "print(\"ğŸ§ª æµ‹è¯•çŠ¶æ€åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"ğŸ“‹ ä¼šè¯ID: {test_state['session_id']}\")\n",
    "print(f\"ğŸ¯ ç›®æ ‡: {test_state['destination']} {test_state['total_days']}å¤©\")\n",
    "print(f\"ğŸ‘¥ å›¢é˜Ÿ: {test_state['user_preferences'].group_size}äºº\")\n",
    "print(f\"ğŸ¨ é£æ ¼: {', '.join(test_state['user_preferences'].travel_style)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ¶æ„éªŒè¯æµ‹è¯•\n",
    "\n",
    "æµ‹è¯•å‰ä¸¤ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œï¼ŒéªŒè¯æ¶æ„å¯è¡Œæ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•å¤æ‚åº¦åˆ†æèŠ‚ç‚¹\n",
    "print(\"=== æµ‹è¯•å¤æ‚åº¦åˆ†æèŠ‚ç‚¹ ===\")\n",
    "test_state_1 = analyze_complexity_node(test_state.copy())\n",
    "print(f\"è¿›åº¦: {test_state_1['progress']}%\")\n",
    "print(f\"é˜¶æ®µ: {test_state_1['current_phase']}\")\n",
    "\n",
    "print(\"\\n=== æµ‹è¯•åŒºåŸŸåˆ†è§£èŠ‚ç‚¹ ===\")\n",
    "test_state_2 = region_decomposition_node(test_state_1)\n",
    "print(f\"è¿›åº¦: {test_state_2['progress']}%\")\n",
    "print(f\"åŒºåŸŸæ•°é‡: {len(test_state_2['regions'])}\")\n",
    "print(f\"å½“å‰åŒºåŸŸç´¢å¼•: {test_state_2['current_region_index']}\")\n",
    "\n",
    "print(\"\\n=== æµ‹è¯•æ¡ä»¶è·¯ç”± ===\")\n",
    "next_step = should_continue_regions(test_state_2)\n",
    "print(f\"ä¸‹ä¸€æ­¥: {next_step}\")\n",
    "\n",
    "print(\"\\nâœ… æ ¸å¿ƒæ¶æ„éªŒè¯å®Œæˆï¼\")\n",
    "print(\"ğŸ“Š éªŒè¯ç»“æœ:\")\n",
    "print(f\"  - çŠ¶æ€ç®¡ç†: âœ… æ­£å¸¸\")\n",
    "print(f\"  - èŠ‚ç‚¹æ‰§è¡Œ: âœ… æ­£å¸¸\")\n",
    "print(f\"  - æ¡ä»¶è·¯ç”±: âœ… æ­£å¸¸\")\n",
    "print(f\"  - Tokené¢„ä¼°: {test_state_2['tokens_remaining']} tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
